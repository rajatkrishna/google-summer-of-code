{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatkrishna/google-summer-of-code/blob/main/notebooks/Export_BERT_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-y2Ucd5abjn"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ52u1xda9Oo"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art open-source deep learning model widely used for a range of NLP tasks including Question Answering, Named Entity Recognition and Sequence Prediction. As opposed to conventional language models that read text sequentially- either left-to-right or right-to-left, BERT is based on the Transformer architecture, which enables bidirectional capabilities.\n",
        "\n",
        "BERT has been pretrained with two objectives:\n",
        "- Masked Language Modeling (MLM) in which a fraction of the words in the input sentences is masked and the model has to predict the masked words.\n",
        "- Next Sentence Prediction (NSP) in which the model learns to predict whether two input sentences are following each other or not.\n",
        "\n",
        "This way, the model learns a meaningful inner representation of the English language that can then be used to produce embeddings. One of the main advantages of BERT is that the vector of a word changes depending on how it is used in a sentence. For instance, BERT produces different embeddings for the word \"bank\" in the following sentences based on its surrounding words:\n",
        "\n",
        "```\n",
        "John works at the bank.\n",
        "```\n",
        "```\n",
        "Robin had to bank on her friend for support.\n",
        "```\n",
        "\n",
        "In this notebook, we will export a BERT model from [HuggingFace](https://huggingface.co/models) in the Tensorflow SavedModel format using Huggingface's `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyzfCvRzkIb9"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNjBp_RBjXBF"
      },
      "source": [
        "We need to install the HuggingFace `transformers` library and Tensorflow in order to download and save the model.\n",
        "\n",
        "Note that these dependencies are only needed to export the model. We do not need them when importing the saved model in Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPvGOkw4i5me"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[tf-cpu]==4.31.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvRMl1twow2o"
      },
      "source": [
        "# TFBert Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmiArCSotqo"
      },
      "source": [
        "Download the BERT model and tokenizer from HuggingFace. The model can be selected from the available BERT models in the `Fill Mask` category. Models trained or fine-tuned on a specific task such as Token Classification cannot be used. In this example, we will use the [bert-base-cased](https://huggingface.co/bert-base-cased) model from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqwr1xd3qMu1"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel, BertTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "MODEL_NAME = 'bert-base-cased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME).save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n",
        "model = TFBertModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLjW8dOfqu0n"
      },
      "source": [
        "Define the model signature before exporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rVBkTZzfqv2j"
      },
      "outputs": [],
      "source": [
        "@tf.function(\n",
        "  input_signature=[\n",
        "      {\n",
        "          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
        "          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n",
        "          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n",
        "      }\n",
        "  ]\n",
        ")\n",
        "def serving_fn(input):\n",
        "    return model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4otwiFXTgriX"
      },
      "source": [
        "# Export in SavedModel format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytS-oIibrTB6"
      },
      "source": [
        "Export the model and weights in the saved model format using the `save_pretrained` function offered by HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rze13N6rrIBN"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(save_directory=\"{}\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZAgWIG_smD3"
      },
      "source": [
        "The saved model can be found in the following folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu01BiIorpn4",
        "outputId": "8e7b7d83-262b-4d4e-a0e0-d054b3dffb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased/saved_model/1:\n",
            "total 6.9M\n",
            "drwxr-xr-x 2 root root 4.0K Aug 28 03:28 assets\n",
            "-rw-r--r-- 1 root root   56 Aug 28 03:28 fingerprint.pb\n",
            "-rw-r--r-- 1 root root 162K Aug 28 03:28 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 6.7M Aug 28 03:28 saved_model.pb\n",
            "drwxr-xr-x 2 root root 4.0K Aug 28 03:28 variables\n",
            "\n",
            "bert-base-cased/saved_model/1/assets:\n",
            "total 0\n",
            "\n",
            "bert-base-cased/saved_model/1/variables:\n",
            "total 414M\n",
            "-rw-r--r-- 1 root root 414M Aug 28 03:28 variables.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  12K Aug 28 03:28 variables.index\n"
          ]
        }
      ],
      "source": [
        "!ls -lhR {MODEL_NAME}/saved_model/1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_qs4kf1H9tG"
      },
      "source": [
        "Copy the `vocabs.txt` file from the tokenizer to the `assets` directory in the saved model directory. These are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oKpo6n7vrdI3"
      },
      "outputs": [],
      "source": [
        "!cp {MODEL_NAME}_tokenizer/vocab.txt {MODEL_NAME}/saved_model/1/assets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFcaRMpkcZe"
      },
      "source": [
        "To import this model in Spark NLP, all you need is the `{MODEL_NAME}/saved_model/1` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fikkm_sgLSCz"
      },
      "source": [
        "# OpenVINOâ„¢ Intermediate Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFcJSt6GK9Yk"
      },
      "source": [
        "To maximize the benefits of OpenVINO Tools, models of various frameworks can be converted into the OpenVINO Intermediate Representation (IR) format- a proprietary model format of OpenVINO. The resulting files can be loaded later using the OpenVINO Runtime. The saved files include\n",
        "- **saved_model.xml**: A file in xml format that describes the model topology\n",
        "- **saved_model.bin**: File containing the model weights and binary data\n",
        "\n",
        "Model conversion API is represented by convert_model() method in openvino.tools.mo namespace. It is included as part of the OpenVINO Development Tools package- a set of utilities that enable users to easily prepare and optimize models for OpenVINO.\n",
        "\n",
        "First, we install the OpenVINO Runtime and Development Tools packages via `pip`. Since the source model is Tensorflow-based, pass the `tensorflow2` argument to automatically install and configure the necessary dependencies for working with Tensorflow 1.x and 2.x models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC7InSpzLRpV"
      },
      "outputs": [],
      "source": [
        "!pip install -q openvino==2023.0.1 openvino-dev[tensorflow2]==2023.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOIv8iu3sduW"
      },
      "source": [
        "With the dependencies set up, we first convert the Tensorflow model into the OpenVINO **ov.Model** format using `convert_model`. During conversion, we can optionally specify the `compress_to_fp16` parameter to compress the constants (for example, weights for matrix multiplications) to **FP16** data type. Half-precision floating point numbers (FP16) have a smaller range, and can result in better performance in cases where half-precision is enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NX3kKJnlPEGf"
      },
      "outputs": [],
      "source": [
        "from openvino.tools.mo import convert_model\n",
        "from openvino.runtime import serialize\n",
        "\n",
        "TF_MODEL_PATH=\"{}/saved_model/1\".format(MODEL_NAME)\n",
        "ov_model = convert_model(TF_MODEL_PATH, compress_to_fp16=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLSVjoeQcG0"
      },
      "source": [
        "To export the converted model into the OpenVINO IR format, the Runtime API provides a `serialize` method that takes in the model in **ov.Model** format and the target path to the resulting model xml file. The accompanying binary file can be found in the same directory.\n",
        "\n",
        "First, we create the directory to save the resulting model files to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rKeCliSDRKAI"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJDUX-yYxEcr"
      },
      "source": [
        "Export the converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LuEzgyvhP9J0"
      },
      "outputs": [],
      "source": [
        "serialize(ov_model, xml_path=\"{}_ir/saved_model.xml\".format(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcJokmXxUz-"
      },
      "source": [
        "Let us take a look at the exported model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8W-CHhRRcaO",
        "outputId": "85bc416c-4de2-4547-93e7-7b2a0bca5369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased_ir:\n",
            "total 208M\n",
            "-rw-r--r-- 1 root root 207M Aug 28 03:32 saved_model.bin\n",
            "-rw-r--r-- 1 root root 875K Aug 28 03:32 saved_model.xml\n"
          ]
        }
      ],
      "source": [
        "!ls -lhR {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaGGoQPxY22"
      },
      "source": [
        "We can see that the converted model now occupies half the disk space as the original. In addition to these files, we also need the vocabulary file `vocab.txt` in the `assets` directory for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X1NONMwtiBX0"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir/assets\n",
        "!cp {MODEL_NAME}_tokenizer/vocab.txt {MODEL_NAME}_ir/assets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK0T7vL_yMvT"
      },
      "source": [
        "The resulting {MODEL_NAME}_ir directory is ready to be imported into Spark NLP."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}