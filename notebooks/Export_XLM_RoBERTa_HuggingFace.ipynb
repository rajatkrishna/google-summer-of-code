{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatkrishna/google-summer-of-code/blob/main/notebooks/Export_XLM_RoBERTa_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSqDOw-XNrV3"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBuDcxZENrV4"
      },
      "source": [
        "XLM RoBerta is a large multi-lingual language model that was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) and achieves state-of-the-arts results on multiple cross lingual benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvRGykM4X-cD"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEaSN7O1YOJ-"
      },
      "source": [
        " We need to install the HuggingFace transformers library and Optimum in order to download and save the model in the ONNX format. In addition to these, XLM-RoBERTa model also needs the `SentencePiece` library to be installed.\n",
        "\n",
        "Note that these dependencies are only needed to export the model. We do not need them when importing the saved model in Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ck9W9qGXsds"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[onnx]==4.27.4 optimum sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytWj7stY3pP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIMxwJFIY5C6"
      },
      "source": [
        "In this example, we will use the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ0OHyM-Yyv1"
      },
      "outputs": [],
      "source": [
        "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
        "\n",
        "MODEL_NAME = 'xlm-roberta-base'\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained(MODEL_NAME, export=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTh-HJ4UbP1i"
      },
      "source": [
        "# Export in SavedModel format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZijM2zbR1n"
      },
      "source": [
        "Export the model and weights in the saved model format using the `save_pretrained` function offered by HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fCAvnB7bQZv"
      },
      "outputs": [],
      "source": [
        "ort_model.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCyJJwPzbbyG"
      },
      "source": [
        "The saved model can be found in the following folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjqA7cxvbXn4",
        "outputId": "62d12730-f82f-48e1-c374-80503ed58fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.1G\n",
            "-rw-r--r-- 1 root root  679 Aug 28 03:40 config.json\n",
            "-rw-r--r-- 1 root root 1.1G Aug 28 03:40 model.onnx\n",
            "-rw-r--r-- 1 root root 4.9M Aug 28 03:40 sentencepiece.bpe.model\n",
            "-rw-r--r-- 1 root root  280 Aug 28 03:40 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  413 Aug 28 03:40 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  17M Aug 28 03:40 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh onnx_models/{MODEL_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh64P-dSNrV8"
      },
      "source": [
        "The resulting `model.onnx` can be imported and run in Spark NLP using Onnx Runtime from Spark NLP 5.0 or the OpenVINO Runtime. We will also need the `sentencepiece.bpe.model` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r1KxzpJSdQfa"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets\n",
        "!cp {EXPORT_PATH}/sentencepiece.bpe.model {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQmUnBsNrV9"
      },
      "source": [
        "# OpenVINOâ„¢ Intermediate Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24uK6pzNrV9"
      },
      "source": [
        "To maximize the benefits of OpenVINO Tools, models of various frameworks can be converted into the OpenVINO Intermediate Representation (IR) format- a proprietary model format of OpenVINO. The resulting files can be loaded later using the OpenVINO Runtime. The saved files include\n",
        "- **msaved_odel.xml**: A file in xml format that describes the model topology\n",
        "- **saved_model.bin**: File containing the model weights and binary data\n",
        "\n",
        "Model conversion API is represented by convert_model() method in openvino.tools.mo namespace. It is included as part of the OpenVINO Development Tools package- a set of utilities that enable users to easily prepare and optimize models for OpenVINO.\n",
        "\n",
        "First, we install the OpenVINO Runtime and Development Tools packages via `pip`. Since the source model is ONNX-based, pass the `onnx` argument to automatically install and configure the necessary dependencies for working with Tensorflow 1.x and 2.x models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzAp4ycfNrV9"
      },
      "outputs": [],
      "source": [
        "!pip install -q openvino==2023.0.1 openvino-dev[onnx]==2023.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsz2PMbONrV9"
      },
      "source": [
        "With the dependencies set up, we first convert the ONNX model into the OpenVINO **ov.Model** format using `convert_model`. During conversion, we can optionally specify the `compress_to_fp16` parameter to compress the constants (for example, weights for matrix multiplications) to **FP16** data type. Half-precision floating point numbers (FP16) have a smaller range, and can result in better performance in cases where half-precision is enough."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openvino.tools.mo import convert_model\n",
        "from openvino.runtime import serialize\n",
        "\n",
        "ONNX_MODEL_PATH=\"{}/model.onnx\".format(EXPORT_PATH)\n",
        "ov_model = convert_model(ONNX_MODEL_PATH, compress_to_fp16=True)"
      ],
      "metadata": {
        "id": "jiPIy1L6PBpY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA6UjNG5NrV9"
      },
      "source": [
        "To export the converted model into the OpenVINO IR format, the Runtime API provides a `serialize` method that takes in the model in **ov.Model** format and the target path to the resulting model xml file. The accompanying binary file can be found in the same directory.\n",
        "\n",
        "First, we create the directory to save the resulting model files to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NECtTU6LNrV9"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dVf-G4NrV9"
      },
      "source": [
        "Export the converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZNoCwtdFNrV-"
      },
      "outputs": [],
      "source": [
        "serialize(ov_model, xml_path=\"{}_ir/saved_model.xml\".format(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da8dRAHjNrV-"
      },
      "source": [
        "Let us take a look at the exported model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht0il3o-NrV-",
        "outputId": "ddb6bae2-5e38-421a-9832-116c53939a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xlm-roberta-base_ir:\n",
            "total 530M\n",
            "-rw-r--r-- 1 root root 530M Aug 28 03:42 saved_model.bin\n",
            "-rw-r--r-- 1 root root 494K Aug 28 03:42 saved_model.xml\n"
          ]
        }
      ],
      "source": [
        "!ls -lhR {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG1NS56GNrV-"
      },
      "source": [
        "We can see that the converted model now occupies half the disk space as the original. Finally, cope the `sentencepiece.bpe.model` file into the assets directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ykvoWhBlNrV-"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir/assets\n",
        "!cp {EXPORT_PATH}/sentencepiece.bpe.model {MODEL_NAME}_ir/assets"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}