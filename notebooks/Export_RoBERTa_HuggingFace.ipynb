{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatkrishna/google-summer-of-code/blob/main/notebooks/Export_RoBERTa_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GicF7VrhMbQR"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO7f9iuRMbQT"
      },
      "source": [
        "RoBERTa is a variation of the BERT model by modifying key hyperparameters, removing the next-sentence prediction pretraining objective and training with larger mini-batches and learning rates. RoBERTa has the same model architecture as BERT, but improves the model performance delivering state-of-the-art performance from better training, more powerful computing, or increased data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSXFRDCxtSHT"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR1-pvSRtWl0"
      },
      "source": [
        " We need to install the HuggingFace transformers library and Optimum in order to download and save the model in the ONNX format.\n",
        "\n",
        "Note that these dependencies are only needed to export the model. We do not need them when importing the saved model in Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl6qXhHQtE_x"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.27.4 optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osTaIdU4th3E"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6icUvNti7P"
      },
      "source": [
        "In this example, we will use the [roberta-base](https://huggingface.co/roberta-base) model pretrained using a masked language modeling (MLM) objective from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGHt1wWbtg2u"
      },
      "outputs": [],
      "source": [
        "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
        "\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained(MODEL_NAME, export=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AoJrkfSuSVW"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZqEJX-4uTjE"
      },
      "source": [
        "Export the model and weights in the ONNX format using the `save_pretrained` function offered by HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U89roa81uPpz"
      },
      "outputs": [],
      "source": [
        "ort_model.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYIbGz6vBMG"
      },
      "source": [
        "The saved model can be found in the following folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxlqEf0fj1Eg",
        "outputId": "f7a8b921-5f4b-4fbf-e309-bf1181fd9c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 477M\n",
            "-rw-r--r-- 1 root root  644 Aug 28 03:36 config.json\n",
            "-rw-r--r-- 1 root root 446K Aug 28 03:36 merges.txt\n",
            "-rw-r--r-- 1 root root 474M Aug 28 03:36 model.onnx\n",
            "-rw-r--r-- 1 root root  280 Aug 28 03:36 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  346 Aug 28 03:36 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 2.1M Aug 28 03:36 tokenizer.json\n",
            "-rw-r--r-- 1 root root 780K Aug 28 03:36 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh onnx_models/{MODEL_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw6lBipfvL1k"
      },
      "source": [
        "The `model.onnx` file represents the exported model. Convert `vocab.json` to `vocab.txt` and copy this file and `merges.txt` file from the tokenizer to the `assets` directory in the saved model directory. These are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z0XI0vlJvKxV"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets\n",
        "\n",
        "vocabs = ort_model.preprocessors[0].get_vocab()\n",
        "vocabs = sorted(vocabs, key=vocabs.get)\n",
        "\n",
        "with open(f'{EXPORT_PATH}/vocab.txt', 'w') as f:\n",
        "    for item in vocabs:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "!cp {EXPORT_PATH}/vocab.txt {EXPORT_PATH}/assets\n",
        "!cp {EXPORT_PATH}/merges.txt {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5NkP547wKoa"
      },
      "source": [
        "To import this model in Spark NLP, all you need is the `onnx_models/{MODEL_NAME}` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYus2RDMbQX"
      },
      "source": [
        "## OpenVINOâ„¢ Intermediate Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aoakqX_MbQX"
      },
      "source": [
        "To maximize the benefits of OpenVINO Tools, models of various frameworks can be converted into the OpenVINO Intermediate Representation (IR) format- a proprietary model format of OpenVINO. The resulting files can be loaded later using the OpenVINO Runtime. The saved files include\n",
        "- **saved_model.xml**: A file in xml format that describes the model topology\n",
        "- **saved_model.bin**: File containing the model weights and binary data\n",
        "\n",
        "Model conversion API is represented by convert_model() method in openvino.tools.mo namespace. It is included as part of the OpenVINO Development Tools package- a set of utilities that enable users to easily prepare and optimize models for OpenVINO.\n",
        "\n",
        "First, we install the OpenVINO Runtime and Development Tools packages via `pip`. Since the source model is ONNX-based, pass the `onnx` argument to automatically install and configure the necessary dependencies for working with ONNX models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "_bOpvw4QMbQY"
      },
      "outputs": [],
      "source": [
        "!pip install -q openvino==2023.0.1 openvino-dev[onnx]==2023.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrRyyaYMbQY"
      },
      "source": [
        "With the dependencies set up, we first convert the ONNX model into the OpenVINO **ov.Model** format using `convert_model`. During conversion, we can optionally specify the `compress_to_fp16` parameter to compress the constants (for example, weights for matrix multiplications) to **FP16** data type. Half-precision floating point numbers (FP16) have a smaller range, and can result in better performance in cases where half-precision is enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "xBAvnnkGMbQY"
      },
      "outputs": [],
      "source": [
        "from openvino.tools.mo import convert_model\n",
        "from openvino.runtime import serialize\n",
        "\n",
        "ONNX_MODEL_PATH=\"{}/model.onnx\".format(EXPORT_PATH)\n",
        "ov_model = convert_model(ONNX_MODEL_PATH, compress_to_fp16=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4pjXItMbQY"
      },
      "source": [
        "To export the converted model into the OpenVINO IR format, the Runtime API provides a `serialize` method that takes in the model in **ov.Model** format and the target path to the resulting model xml file. The accompanying binary file can be found in the same directory.\n",
        "\n",
        "First, we create the directory to save the resulting model files to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "tl_4kKfbMbQZ"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_kZ_6_MbQZ"
      },
      "source": [
        "Export the converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "mNMkUr_fMbQZ"
      },
      "outputs": [],
      "source": [
        "serialize(ov_model, xml_path=\"{}_ir/saved_model.xml\".format(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxWqFwkvMbQZ"
      },
      "source": [
        "Let us take a look at the exported model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ6l1O9gMbQZ",
        "outputId": "979e0bc7-ada4-43c2-eba3-45afc20e5bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta-base_ir:\n",
            "total 238M\n",
            "-rw-r--r-- 1 root root 237M Aug 28 03:38 saved_model.bin\n",
            "-rw-r--r-- 1 root root 494K Aug 28 03:38 saved_model.xml\n"
          ]
        }
      ],
      "source": [
        "!ls -lhR {MODEL_NAME}_ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emN7Qj72MbQZ"
      },
      "source": [
        "We can see that the converted model now occupies half the disk space as the original. In addition to these, we need the `vocab.txt` and `merges.txt` file from the tokenizer to the `assets` directory in the saved model directory. These are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "M8dHa4smMbQZ"
      },
      "outputs": [],
      "source": [
        "!mkdir {MODEL_NAME}_ir/assets\n",
        "!cp {EXPORT_PATH}/vocab.txt {MODEL_NAME}_ir/assets\n",
        "!cp {EXPORT_PATH}/merges.txt {MODEL_NAME}_ir/assets"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}